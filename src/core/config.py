from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Optional

class Settings(BaseSettings):
    # To load from .env file, ensure python-dotenv is installed
    # and pydantic_settings will automatically load it.
    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')

    OLLAMA_BASE_URL: str = "http://localhost:11434"
    PRIMARY_MODEL_NAME: str = "llama3" # Default primary model
    
    # Future settings for Model 2 (Critique Model)
    CRITIQUE_MODEL_NAME: Optional[str] = "llama3" # Default critique model, can be same or different
    CRITIQUE_SYSTEM_PROMPT: Optional[str] = ( # Default system prompt for critique model
        "You are an expert critique and refinement AI. "
        "Your task is to improve a response generated by another AI assistant (Model 1) for a given user query.\n\n"
        "The original user query was:\n"
        "---USER QUERY START---\n"
        "{original_user_query}\n"
        "---USER QUERY END---\n\n"
        "Model 1 (the initial assistant) provided the following response:\n"
        "---MODEL 1 RESPONSE START---\n"
        "{model_1_response_content}\n"
        "---MODEL 1 RESPONSE END---\n\n"
        "Critique Model 1's response based on accuracy, completeness, clarity, conciseness, and adherence to best practices (especially if it's code). "
        "Then, generate a new, improved response that directly answers the original user query, addressing any flaws you found. "
        "Your output should ONLY be the refined response, suitable to be sent back to the user as if you were the primary assistant. "
        "Do NOT include any preamble like 'Here is the refined response:'."
    )

    # Logging level, e.g., INFO, DEBUG
    LOG_LEVEL: str = "INFO"

settings = Settings()
