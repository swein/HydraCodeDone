# HydraCodeDone: LLM Critique Proxy

This project provides a self-hosted FastAPI proxy server designed to integrate with local Large Language Models (LLMs) via Ollama. It implements a dual-model critique pipeline: an initial response is generated by a primary LLM, and then a second LLM (the critique model) refines or critiques this response. The proxy aims for OpenAI API compatibility for seamless integration with various IDEs and tools.

## Features

- **OpenAI API Compatibility**: Exposes a `/v1/chat/completions` endpoint that mirrors the OpenAI API, allowing easy integration with tools expecting this format. Tested with Roo cline, continue.dev, open webui.
- **Dual-Model Critique Pipeline**: An initial response is generated by a primary LLM. A second LLM (the critique model) then refines this response. The critique model is guided by a detailed system prompt and the full context of the original request, enabling it to improve accuracy, adhere to formatting, and produce output suitable for direct IDE/tool consumption.
- **Official OpenAI Pydantic Models**: Leverages Pydantic models from the official `openai` Python library (v1.0+) for request validation and response structuring, ensuring type safety and compatibility.
- **Dockerized**: Runs as a Docker container using Docker Compose, including an Ollama service for managing local LLMs.
- **Configurable**: Uses environment variables for easy configuration of models, prompts, and logging.
- **Streaming Responses**: Supports the OpenAI `stream` parameter and returns Server-Sent Events (SSE), enabling real-time token streaming in Continue.dev, OpenWebUI, and other clients.

## Critique Model Behavior Note

It's important to note that the critique model (Model 2), guided by the `CRITIQUE_SYSTEM_PROMPT`, typically provides the core refined content. For instance, if Model 1's response includes conversational preamble or specific structural formatting (like markdown code blocks with file paths as seen in some IDE prompts), Model 2's refined output will often be the essential content itself (e.g., just the code), having stripped away the surrounding elements. This is by design to provide a clean, direct output for tools that consume the API.

## Prerequisites
- Python 3.11+ (pytest for testing)
- Docker
- Docker Compose (V2 recommended, i.e., `docker compose` command)

## Environment Variables

Create a `.env` file in the project root by copying `.env.example` (if provided) or creating it from scratch. Populate it with the following variables:

- `OLLAMA_BASE_URL`: The base URL for the Ollama service. This should be `http://your_ollama_service:11434`.
- `PRIMARY_MODEL_NAME`: The name of the Ollama model to be used for generating the initial response (e.g., `llama3.2`).
- `CRITIQUE_MODEL_NAME`: The name of the Ollama model to be used for critiquing the initial response (e.g., `deepseek-r1:8b`). If commented out or empty, the critique step will be skipped.
- `CRITIQUE_SYSTEM_PROMPT`: The detailed system prompt used to guide the critique model. This prompt instructs Model 2 on how to analyze Model 1's response in the context of the entire original user request, focusing on correctness, completeness, and adherence to any implicit or explicit formatting requirements from the original request. The goal is for Model 2 to produce a polished, final response. The actual prompt is multi-line and should be defined in your `.env` file (see example below).
- `LOG_LEVEL`: The logging level for the application (e.g., `INFO`, `DEBUG`). Defaults to `INFO`.

Example `.env` file:

```
OLLAMA_BASE_URL="http://ollama_service:11434"
PRIMARY_MODEL_NAME="llama3.2"
CRITIQUE_MODEL_NAME="deepseek-r1:8b"
CRITIQUE_SYSTEM_PROMPT="You are now in a critique and refinement phase.\nBased on the entire preceding conversation, including the user's original request and the last AI's response:\n1. Identify areas for improvement in the LAST AI's response. Focus on:\n   - Correcting bugs, syntax errors, and typos.\n   - Addressing logic issues.\n   - Enhancing clarity, conciseness, and overall quality.\n   - Ensuring the response fully addresses the user's original query.\n2. Provide a revised and improved response.\n3. CRUCIAL: Your revised response MUST strictly adhere to any output formatting, structural requirements, or specific instructions implied by the user's original request(s) earlier in the conversation.\nYour goal is to produce a polished version suitable for direct use by the user's IDE/tool.\nPlease provide ONLY the final, refined response according to these instructions."
LOG_LEVEL="INFO"
```

## Running the Application

1.  **Ensure Docker is running.**
2.  **Navigate to the project root directory** in your terminal.
3.  **Build and start the services** using Docker Compose:
    ```bash
    docker compose up -d --build
    ```
    This command will build the `llm_proxy_service` image and start it.
4.  The LLM proxy service will be available at `http://localhost:3101` (or the port configured in `docker-compose.yaml`).

To stop the services:
```bash
docker compose down
```

## API Endpoints

- **`GET /health`**: Basic liveness probe.
- **`GET /v1/health`**: OpenAI-style health probe used by Continue.dev (returns the same `{"status":"ok"}`).
- **`GET /v1/models`**: Lists available model IDs in OpenAI *List Models* format.

- **`POST /v1/chat/completions`**: OpenAI-compatible chat completions endpoint.
  - **Request Body**: Follows the OpenAI ChatCompletion API schema (e.g., `model`, `messages` array).
  - **Response Body**: Follows the OpenAI ChatCompletion API schema, including choices and usage (usage stats are currently placeholders).
- **`POST /chat/completions`**: Alias without `/v1` prefix for clients that call it directly.

## Streaming Usage

The proxy fully supports streaming in the same way as the OpenAI API. Set `"stream": true` in your request and the response will be delivered as a `text/event-stream` where each line begins with `data:` followed by a JSON chunk. The stream terminates with `data: [DONE]`.

Example `curl` request:

```bash
curl -X POST "http://localhost:3101/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "'$PRIMARY_MODEL_NAME'",  /* replace or export env var */
    "stream": true,
    "messages": [
      {"role": "user", "content": "What is Ruby?"}
    ]
  }'
```

## Testing

Unit and integration tests (covering both non-streaming and streaming code paths) are written using `pytest`. The suite mocks Ollama HTTP calls with `respx` and patches streaming generators for fast, isolated testing.

1.  **Ensure your virtual environment is activated** and development dependencies are installed:
    ```bash
    python -m venv .venv
    source .venv/bin/activate
    pip install -r requirements.txt
    pip install -r requirements-dev.txt # For pytest and other dev tools
    ```
2.  **Run tests** from the project root:
    ```bash
    .venv/bin/python -m pytest
    ```
    Or, if your `PATH` is set up correctly after activating the venv:
    ```bash
    pytest
    ```
