# HydraCodeDone: LLM Critique Proxy

This project provides a self-hosted FastAPI proxy server designed to integrate with local Large Language Models (LLMs) via Ollama. It implements a dual-model critique pipeline: an initial response is generated by a primary LLM, and then a second LLM (the critique model) refines or critiques this response. The proxy aims for OpenAI API compatibility for seamless integration with various IDEs and tools.

## Features

- **OpenAI API Compatibility**: Exposes a `/v1/chat/completions` endpoint that mirrors the OpenAI API, allowing easy integration with tools expecting this format.
- **Dual-Model Critique Pipeline**: An initial response is generated by a primary LLM. A second LLM (the critique model) then refines this response. The critique model is guided by a detailed system prompt and the full context of the original request, enabling it to improve accuracy, adhere to formatting, and produce output suitable for direct IDE/tool consumption.
- **Official OpenAI Pydantic Models**: Leverages Pydantic models from the official `openai` Python library (v1.0+) for request validation and response structuring, ensuring type safety and compatibility.
- **Dockerized**: Runs as a Docker container using Docker Compose, including an Ollama service for managing local LLMs.
- **Configurable**: Uses environment variables for easy configuration of models, prompts, and logging.

## Critique Model Behavior Note

It's important to note that the critique model (Model 2), guided by the `CRITIQUE_SYSTEM_PROMPT`, typically provides the core refined content. For instance, if Model 1's response includes conversational preamble or specific structural formatting (like markdown code blocks with file paths as seen in some IDE prompts), Model 2's refined output will often be the essential content itself (e.g., just the code), having stripped away the surrounding elements. This is by design to provide a clean, direct output for tools that consume the API.

## Prerequisites

- Docker
- Docker Compose (V2 recommended, i.e., `docker compose` command)

## Environment Variables

Create a `.env` file in the project root by copying `.env.example` (if provided) or creating it from scratch. Populate it with the following variables:

- `OLLAMA_BASE_URL`: The base URL for the Ollama service. When running with the provided Docker Compose setup, this should be `http://ollama_service:11434`.
- `PRIMARY_MODEL_NAME`: The name of the Ollama model to be used for generating the initial response (e.g., `llama3.2`).
- `CRITIQUE_MODEL_NAME`: The name of the Ollama model to be used for critiquing the initial response (e.g., `llama3.2`). If commented out or empty, the critique step will be skipped.
- `CRITIQUE_SYSTEM_PROMPT`: The detailed system prompt used to guide the critique model. This prompt instructs Model 2 on how to analyze Model 1's response in the context of the entire original user request, focusing on correctness, completeness, and adherence to any implicit or explicit formatting requirements from the original request. The goal is for Model 2 to produce a polished, final response. The actual prompt is multi-line and should be defined in your `.env` file (see example below).
- `LOG_LEVEL`: The logging level for the application (e.g., `INFO`, `DEBUG`). Defaults to `INFO`.

Example `.env` file:

```
OLLAMA_BASE_URL="http://ollama_service:11434"
PRIMARY_MODEL_NAME="llama3.2"
CRITIQUE_MODEL_NAME="llama3.2"
CRITIQUE_SYSTEM_PROMPT="You are now in a critique and refinement phase.\nBased on the entire preceding conversation, including the user's original request and the last AI's response:\n1. Identify areas for improvement in the LAST AI's response. Focus on:\n   - Correcting bugs, syntax errors, and typos.\n   - Addressing logic issues.\n   - Enhancing clarity, conciseness, and overall quality.\n   - Ensuring the response fully addresses the user's original query.\n2. Provide a revised and improved response.\n3. CRUCIAL: Your revised response MUST strictly adhere to any output formatting, structural requirements, or specific instructions implied by the user's original request(s) earlier in the conversation.\nYour goal is to produce a polished version suitable for direct use by the user's IDE/tool.\nPlease provide ONLY the final, refined response according to these instructions."
LOG_LEVEL="INFO"
```

## Running the Application

1.  **Ensure Docker is running.**
2.  **Navigate to the project root directory** in your terminal.
3.  **Build and start the services** using Docker Compose:
    ```bash
    docker compose up --build
    ```
    This command will build the `llm_proxy_service` image and pull the `ollama/ollama` image if they don't exist locally. It will then start both services.

4.  **Pull necessary Ollama models**: Once the `ollama_service` is running, you need to pull the models specified in your `.env` file. You can do this by executing commands inside the `ollama_service` container. For example, to pull `llama3.2`:
    ```bash
    docker compose exec ollama_service ollama pull llama3.2
    ```
    Repeat for `CRITIQUE_MODEL_NAME` if it's different.

5.  The LLM proxy service will be available at `http://localhost:3101` (or the port configured in `docker-compose.yaml`).

To stop the services:
```bash
docker compose down
```

## API Endpoints

- **`GET /health`**: Returns the health status of the application.
  - Response: `{"status":"ok"}`

- **`POST /v1/chat/completions`**: OpenAI-compatible chat completions endpoint.
  - **Request Body**: Follows the OpenAI ChatCompletion API schema (e.g., `model`, `messages` array).
  - **Response Body**: Follows the OpenAI ChatCompletion API schema, including choices and usage (usage stats are currently placeholders).

## Testing

Unit and integration tests are written using `pytest`.

1.  **Ensure your virtual environment is activated** and development dependencies are installed:
    ```bash
    python -m venv .venv
    source .venv/bin/activate
    pip install -r requirements.txt
    pip install -r requirements-dev.txt # For pytest and other dev tools
    ```
2.  **Run tests** from the project root:
    ```bash
    .venv/bin/python -m pytest
    ```
    Or, if your `PATH` is set up correctly after activating the venv:
    ```bash
    pytest
    ```
